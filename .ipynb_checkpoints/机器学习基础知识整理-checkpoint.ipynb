{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习基础知识总结\n",
    "\n",
    "通过面试题进一步理解机器学习基础知识：）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 面试Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 已掌握机器学习算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、线性回归，逻辑回归 \n",
    "\n",
    "2、SVM\n",
    "\n",
    "3、决策树，随机森林，GBDT\n",
    "\n",
    "4、朴素贝叶斯\n",
    "\n",
    "5、KMeans\n",
    "\n",
    "6、集成方法 Bending、Bagging、Boosting、Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据验证方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练集：训练模型\n",
    "\n",
    "测试集：评估模型的泛化能力\n",
    "\n",
    "验证集：用来选择模型的超参数，比如SVM中的C，r\n",
    "\n",
    "\n",
    "交叉验证方法：\n",
    "\n",
    "1、K折交叉验证：\n",
    "\n",
    "    1、训练集平均分成K份（10） \n",
    "    \n",
    "    2、取第K份作为验证集，剩下K-1份作为训练集 \n",
    "    \n",
    "    3、K次得到的分数平均 = 验证集的分数\n",
    "    \n",
    "    优点：验证集多 -> 有效避免过拟合和欠拟合的发生，更有说服力\n",
    "    \n",
    "2、留一法（leave one out）：\n",
    "\n",
    "    1、一个数据（验证集） +  其他所有的数据（训练集）| N次\n",
    "    \n",
    "    2、再求平均\n",
    "    \n",
    "    优点：模型几乎等于实际训练集训练出的模型\n",
    "    \n",
    "    缺点：1、计算成本大\n",
    "    \n",
    "    用处：SVM快速安全检查  Err <= （支撑向量的个数 / 总样本数量）\n",
    "\n",
    "\n",
    "调参：\n",
    "\n",
    "    网格搜索（K-Fold）：不同的参数组合分别做一次交叉验证，取验证分数最高的 = 最优参数\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 工程面试题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit和fit_transform的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据某种同一处理，标准化等\n",
    "\n",
    "1、先fit，再transform：可以拿到数据的变换（标准化等）的参数，在测试集做一次相同的变换处理\n",
    "\n",
    "2、fit_transform 快，一次性完成数据的变换。但如果训练集和测试集上用fit_transform，可能执行两套变换标准\n",
    "\n",
    "后transform（）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.julyedu.com/question/big/kp_id/23/ques_id/1050"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原因： 1、减少特征数量、降维，使模型泛化能力更强，减少过拟合\n",
    "\n",
    "      2、增强可解释性 【预测房价】\n",
    "      \n",
    "方法：\n",
    "      1、正则： L1正则化可以产生稀疏的模型\n",
    "      \n",
    "      2、去除方差小的特征。方差小 = 意味着每个样本这个特征基本上没啥太大区别，对结果影响小\n",
    "      \n",
    "      3、随机森林做特征选择：分支时 = 【分类】信息增益/基尼不纯度最大\n",
    "      \n",
    "      4、结合业务理解，探索性数据分析，可解释性更强 【预测房价，房价跟面积数值变量 = 散点图； 房屋材料评估 = 箱型图】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估指标 ✔️✔️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理 ✔️✔️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4C：\n",
    "\n",
    "正确性（异常值：1.5四分位距 | 双变量散点图）\n",
    "\n",
    "完整性（None空值： 1、空值的业务意义  2、空值限定上限 3、太多去掉 4、数值：中位数；类别：众数）\n",
    "\n",
    "创造性\n",
    "\n",
    "转换性（类别特征独热编码； 数值特征 1，2，3转化为类别特征； 连续特征离散化（有些模型：逻辑回归需要离散值，表达能力））\n",
    "\n",
    "皮尔逊相关系数矩阵 【预测房价seaborn】去除相关性高的特征之一\n",
    "\n",
    "数据正态分布为什么：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 选择分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "吴恩达：\n",
    "\n",
    "1、特征比数据量还大，很大： 稀疏 -> 线性分类器，LR + Linear Kernel SVM\n",
    "\n",
    "2、特征比较小，样本数量多 ：  +手工添加特征  +非线性分类器\n",
    "\n",
    "3、特征小，样本量一般： SVM + 高斯Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常见的优化算法，优缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、梯度下降法  慢\n",
    "\n",
    "\n",
    "2、随机梯度下降， 每次只选择一个样本 | 损失一小部分精确度，总体效率优化， 全局最优解附近，每次朝全局最优解的方向优化\n",
    "\n",
    "\n",
    "3、牛顿法：\n",
    "\n",
    "直观理解：想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。（牛顿法目光更加长远，所以少走弯路；相对而言，梯度下降法只考虑了局部的最优，没有全局思想。\n",
    "\n",
    "几何理解： 二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径\n",
    "\n",
    "\n",
    "\n",
    "4、解决约束优化问题——拉格朗日乘数法：\n",
    "\n",
    "基本思想：通过引入拉格朗日乘子来将含有n个变量和k个约束条件的约束优化问题 => 含有（n+k）个变量的无约束优化问题\n",
    "\n",
    "k个约束条件 => k个拉格朗日乘子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 给你分配了一个新的项目，是关于帮助食品配送公司节省更多的钱。问题是，公司的送餐队伍没办法准时送餐。结果就是他们的客户很不高兴。\n",
    "\n",
    "最后为了使客户高兴，他们只好以免餐费了事。哪个机器学习算法能拯救他们？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习解决问题：\n",
    "\n",
    "1、已有数据\n",
    "\n",
    "2、模式已经存在\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA降维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解释方差比：该成分方差占总方差的比例，越大，该成分越重要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 面试题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单介绍下LR  ✔️✔️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR = 逻辑回归\n",
    "\n",
    "线性二分类 | softmax多分类\n",
    "\n",
    "1、公式，模拟实际\n",
    "\n",
    "2、判别，判断时\n",
    "\n",
    "3、损失函数，最大似然函数\n",
    "\n",
    "4、梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# penalty: 正则化 l1 / l2\n",
    "# solver : {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’},"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逻辑回归为什么要对特征进行离散化。（独热编码）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、表达能力：LR 线性模型，原来分类线是一条直线y = kx。特征离散化后，原来有一个特征x， x按区间离散化为x_1,x_2,x_3, 相对于y对A的函数相当于分段线性函数。引入了非线性\n",
    "\n",
    "2、健壮性：减少异常值的干扰。 > 30是1，否则为0。如果出现异常值年龄为300，原来 y = kx，异常值对最后结果影响很大；\n",
    "\n",
    "    而离散化后，300也是1，对结果影响较少。\n",
    "\n",
    "3、稳定性：比如年龄x在（20 - 30）为一个区间，不会因为用户增长一岁变成一个完全不同的人\n",
    "\n",
    "4、离散 = 0 / 1传给逻辑回归模型，稀疏向量。内积速度快，方便存储\n",
    "\n",
    "5、降低过拟合的风险。\n",
    "\n",
    "y = k1x1 + k2x2 ，如果x1的权重k1很大，模型过度依赖此特征x1，特征的一个微小变化可能会导致结果产生较大的变化。遇到新样本时可能因为特征的过度敏感得到错的分类结果。\n",
    "\n",
    "离散特征后，一个特征变成多个，权重也变为多个，之前连续特征对模型的影响力被弱化，降低了过拟合的风险。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR和SVM的联系和区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "共同点：\n",
    "\n",
    "    1、都用于分类，一般都用于二分类\n",
    "    2、不同的正则化项，如L1，L2\n",
    "    \n",
    "\n",
    "区别：\n",
    "\n",
    "    ❌1、损失函数不同（优化目标）：逻辑回归：最大似然函数，求w使出现样本情况的概率最大   SVM ：\n",
    "    \n",
    "    ❌2、优化方法不同：逻辑回归：梯度下降；  SVM：KKT条件，必要条件 + 拉格朗日乘子：对于an > 0，支撑向量，计算w和b\n",
    "    \n",
    "    2、几何意义：LR模型找超平面，尽量让所有的点远离这个超平面； SVM寻找最胖的超平面，让最靠近这个超平面的点远离。\n",
    "    \n",
    "    3、SVM只考虑支持向量（局部），也就是和分类最相关的少数点； 逻辑回归考虑全局。\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR和线性回归的区别和联系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "联系：都是广义的线性回归\n",
    "\n",
    "区别：1、线性回归计算的是值，实数范围内； 逻辑回归算的是概率【0，1】，减少预测范围，健壮性更好。\n",
    "\n",
    "     2、线性回归优化目标是最小化最小二乘函数，也就是每个点到超平面的距离最小\n",
    "     \n",
    "       逻辑回归的目标是最大化似然函数，也就是选取w，确定特征x下的概率，使产生训练集这样资料的概率最大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR和随机森林的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、LR是线性的，随机森林等树算法都是非线性的，\n",
    "\n",
    "2、LR全局的最优，🌲模型局部最优，切branch\n",
    "\n",
    "3、LR 梯度下降，去量纲，归一化。 🌲 模型数值不需要。\n",
    "\n",
    "4、优化过拟合的方法，LR正则化。随机森林：Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM的原理 ✔️✔️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM 分类 kernel非线性 |  SVR回归\n",
    "\n",
    "1、标准SVM\n",
    "\n",
    "2、对偶SVM\n",
    "\n",
    "3、Kernel SVM\n",
    "\n",
    "4、"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 说说你知道的核函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、高斯Kernel  \n",
    "\n",
    "2、多项式核函数\n",
    "\n",
    "3、线性核函数\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 带Kernel的SVM为什么能分类非线性问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.julyedu.com/question/big/kp_id/23/ques_id/1566"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM对偶的概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、原来基础SVM特征转换，低维 -> 高维时 -> QP求解【维度 + 1】的变量，复杂\n",
    "\n",
    "2、目的：去除对样本特征转换维度的依赖\n",
    "\n",
    "3、方法：利用**拉格朗日对偶性**将原始问题转化为对偶问题来求解（对偶问题 QP解样本量有关的变量）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树 ✔️✔️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 什么是决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "条件式的学习方法\n",
    "\n",
    "ID、C4.5分类\n",
    "\n",
    "CART 分类和回归\n",
    "\n",
    "1、模拟人决策的过程，整体是一个条件判断的集合。\n",
    "\n",
    "2、分枝节点：条件判断规则\n",
    "\n",
    "3、切分支时：特征空间划分成互不相交的部分。每次根据一个特征进行切分：\n",
    "\n",
    "    1、ID3中：按照不同的特征划分，选择划分后信息增益最大的那个特征进行切分 | 特征值有几种，切几个分支，多叉🌲\n",
    "    \n",
    "    2、CART： 按照不同特征划分，选择划分后基尼不纯度最小的那个特征进行切分 |  二叉🌲\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优点： 1、模型可解释性强（相比于SVM 高斯Kernel等）\n",
    "\n",
    "      2、轻易处理多分类问题 + 高效的非线性模型\n",
    "\n",
    "      3、特征：可以同时处理数值特征和类别特征\n",
    "      \n",
    "      4、代理分支：C&RT容易处理预测中缺失值\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "缺点：1、容易过拟合，完全长成的🌲。\n",
    "\n",
    "解决方法：1、剪枝：前剪枝和后剪枝\n",
    "        \n",
    "        2、集成方法：bagging，随机森林\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常用参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# 默认C&RT\n",
    "\n",
    "# max_features：划分时考虑的最大特征数：控制决策树的生成时间，一般不变\n",
    "\n",
    "# 1、前剪枝\n",
    "\n",
    "# max_depth：   限制树的最大深度：样本多，特征多：10 - 100\n",
    "# min_samples_split： 样本数 < minXX：不会继续选择最优特征划分\n",
    "# max_leaf_nodes：最大叶子节点数\n",
    "# min_impurity_split：如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值\n",
    "\n",
    "\n",
    "# 2、后剪枝\n",
    "\n",
    "# min_samples_leaf：如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "递归的过程：\n",
    "\n",
    "1、接受训练集\n",
    "\n",
    "构建决策树函数：\n",
    "\n",
    "    一、：学习\n",
    "    \n",
    "    1、【学习分支】b(x)\n",
    "    \n",
    "    2、根据分支划分训练集 【分支数量】\n",
    "    \n",
    "    3、构建子树 <- 根据不同划分后的训练集\n",
    "    \n",
    "    4、返回整体的树🌲\n",
    "\n",
    "    \n",
    "    二：【停止条件】：\n",
    "    \n",
    "        返回叶子节点【基础假设】\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  具体关键的4个步骤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、分支数量：ID3 特征值数量     \n",
    "\n",
    "           C&RT：2 二叉树\n",
    "\n",
    "2、基础假设：返回常数。 \n",
    "\n",
    "            分类：最多的类别；\n",
    "            \n",
    "            回归：yn的平均值\n",
    "\n",
    "3、学习分支：看一个特征\n",
    "\n",
    "          1、ID3：特征每个值切割，信息增益最大  \n",
    "         \n",
    "          熵：信息的混乱程度 -SUM(pi * logPi)  信息增益：划分前熵 - 划分后每一个数据集的熵的和\n",
    "          \n",
    "          信息增益：对于多值特征（user_id），按照信息增益切分各部分几乎都是纯分类（熵最小为0），但没有意义。\n",
    "          \n",
    "          一般倾向于选择特征值多的特征切割\n",
    "          \n",
    "          \n",
    "          \n",
    "          2、C4.5信息增益比：信息增益 * 惩罚系数\n",
    "          \n",
    "          信息增益比本质： 是在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。\n",
    "          \n",
    "          惩罚系数：数据集以某一特征作为划分时熵的倒数：特征取值较小，倒数大；； 特征取值较大，倒数小\n",
    "          \n",
    "          【【【【选择】】】】：从候选特征中找出信息增益 > 平均水平 -> 选择信息增益比最高的\n",
    "          \n",
    "          \n",
    "\n",
    "\n",
    "         3、C&RT：切一刀分成两份\n",
    "          \n",
    "          特征类型（切什么）：\n",
    "          \n",
    "          离散特征：决策子集，二分离散特征。A1，A2，A3 =》 （A1A2，A3） （A1，A2A3）（A1A3，A2）\n",
    "          \n",
    "          连续特征：连续特征从小到大排序 -> 每一个中位点 -> 分支从中位点两侧分开 -> 计算判断标准 -> 最合理切分\n",
    "          \n",
    "          \n",
    "          切的判断标准（怎么切）：\n",
    "          \n",
    "          分类：基尼不纯度：数据集中随机抽取两个样本，类别不一致的概率 | 好处：计算简单\n",
    "          \n",
    "          回归：回归误差：SUM（每一个y减去整体的平均值的平方）\n",
    "          \n",
    "          \n",
    "          \n",
    "4、停止条件：\n",
    "\n",
    "     1、所有训练数据子集被正确分类\n",
    "\n",
    "     Ein = 0； 过拟合\n",
    "     \n",
    "     2、前剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树正则化：剪枝 （交叉验证）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、前剪枝：让决策树提前停止\n",
    "\n",
    "优缺点：简洁，节省计算 +  高度和阈值参数设置困难 + 可能欠拟合\n",
    "    \n",
    "       1、设置🌲的高度限制\n",
    "       \n",
    "       2、叶子结点的个数 < 【阈值】\n",
    "\n",
    "       3、信息增益/基尼不纯度 < 【阈值】\n",
    "       \n",
    "       4、每次划分时判断验证集的精度：如果验证集精度提升 -> 划分； 如果验证集精度不提升 -> 不划分 （基于贪心）\n",
    "       \n",
    "\n",
    "2、后剪枝：\n",
    "\n",
    "优缺点：得到完全长成的🌲后【自底向上】对分支结点进行考察处理 ，效果好  +  耗计算资源，时间开销大\n",
    "\n",
    "       1、【自底向上】考察每一个分支结点\n",
    "       \n",
    "       2、分支结点类别 = 对应子树中大多数训练样本所属的类别代替\n",
    "       \n",
    "       3、评估验证集的准确度：提高 -> 剪枝；；不提高 -> 不剪枝\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测中的缺失值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C&RT🌲中包括 **代理分支**：\n",
    "\n",
    "    1、训练时为每个特征找替代特征，b1(x)、b2(x)存起来：用替代特征做切割时跟原来类似\n",
    "    \n",
    "    2、预测时允许特征存在缺失值，使用替代特征代替"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 集成方法 ✔️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging、Boosting、Blending、Stacking的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/qq_30189255/article/details/51532442"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "抽样、学习、集成\n",
    "\n",
    "#### Bagging:  \n",
    "\n",
    "Bootstrap随机抽样 -> 分别训练基模型 -> 均匀投票\n",
    "\n",
    "1、降低方差  2、每个样本权重相同  3、可并行，每个基模型独立，互不影响\n",
    "\n",
    "#### Boosting：\n",
    "\n",
    "根据权重u抽取样本 -> 训练基模型 -> 得到（误差e1，更新权重系数a1）->\n",
    "\n",
    "根据a1确定第二次抽取样本权重u(t+1)。。。 -> 根据e1集成各个基模型\n",
    "\n",
    "#### adaboost（具体）: 前一个基模型上所犯的错误不断放大\n",
    "\n",
    "开始: u(1) -> 1/N\n",
    "\n",
    "更新权重：1、样本权重：做出不同的基模型 -> 让gt在u(t+1)表现不好：犯错的点 / 全部点 = 1/2 ->  确定◇t，使：本次犯错的点数 * 权重（下一次犯错点权重） = 不犯错点数 * 权重（下一次不错点的权重） ->  ◇t \n",
    "\n",
    "集成：at = ln（◇t）\n",
    "\n",
    "1、降低偏差，每一次训练基模型，都是取的在前一次基模型上表现不好的样本，不断调整\n",
    "2、序列式  \n",
    "\n",
    "物理意义：+感知机，只能水平/垂直的线\n",
    "\n",
    "#### Blending：已知gt - > 均匀Blending / 线性Blending（每个gt不同权重）/ 不同条件不同权重\n",
    "\n",
    "1、降低方差，稳定   \n",
    "\n",
    "#### Stacking：\n",
    "\n",
    "1、更加高效  2、可能过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking （Kaggle上预测效果出奇的好）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://blog.kaggle.com/2017/06/15/stacking-made-easy-an-introduction-to-stacknet-by-competitions-grandmaster-marios-michailidis-kazanova/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理解：**对不同模型的集成策略**\n",
    "一般2层stack\n",
    "\n",
    "不同模型：损失函数 - 不同的角度和目标\n",
    "\n",
    "SVM - 几何上最大间距，支持向量\n",
    "\n",
    "决策树 - 关注分裂节点不纯度的变化，整体信息的混乱程度\n",
    "\n",
    "RF - 🌲模型更加全面：增加随机性，尽可能塑造不同的🌲，提高准确度，减少方差\n",
    "\n",
    "GBDT - 梯度下降的角度，选择最快下降的g\n",
    "\n",
    "LR - 全部点是否分类正确，最大似然函数\n",
    "\n",
    "朴素贝叶斯 - 条件概率，出现当前数据的概率最大\n",
    "\n",
    "KNN - 样本点之间的距离，依赖近似点\n",
    "\n",
    "【三个诸葛亮】🕵🕵🕵\n",
    "\n",
    "不同种类的基模型\n",
    "\n",
    "缺点：对一个基模型训练5次，时间消耗大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原理：\n",
    "\n",
    "1、训练：KFold -> 基本Model特征集 -> 元模型 -> 实际值\n",
    "\n",
    "2、测试：每一折预测，平均 -> 基本Model特征集 -> 元模型预测\n",
    "\n",
    "不同数据角度的模型进行融合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机森林 ✔️✔️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**bagging**（不一样，有共同点） + 决策树\n",
    "\n",
    "三个延伸：\n",
    "\n",
    "1、特征端随机抽取：C&RT🌲中每一次做branch的时候取新的特征子空间 = 更不一样的🌲，效率\n",
    "\n",
    "2、**低维空间的投影矩阵**：包含随机子空间 + 随机的方向 （可以切斜线，更复杂，更有能力）\n",
    "\n",
    "3、OOB：没有被Bagging过程选中的资料  1/3\n",
    "\n",
    "        用来验证 （类似留一法）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优点：1、因为低维空间投影矩阵 -> 能够处理高维度的数据，更复杂和有力\n",
    "\n",
    "     2、因为OOB，自我验证，高效      \n",
    "     \n",
    "     3、因为置换检验和OOB，训练完后可以给出特征的重要性\n",
    "\n",
    "缺点：1、回归 -> 不能给出连续的输出 + 不能给出超越训练集数据范围的预测 -> 有可能在一些误差较大的回归问题上过拟合\n",
    "\n",
    "     2、随机性，无法准确控制模型内部的运行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机森林如何评估特征重要性？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、通过特征重要性进行特征选择\n",
    "\n",
    "2、置换检验：\n",
    "\n",
    "    随机测试：如果特征重要，对表现影响大 -> 如果特征改为随机值时，很大影响表现\n",
    "    \n",
    "    1、置换检验：原来样本中某一个特征的值随机排序，洗牌\n",
    "    \n",
    "    2、特征的重要性 = 原资料表现 - 置换检验后资料的表现\n",
    "    \n",
    "    3、 = Eoob(G) - Eoob(G【排序后】） = Eoob(G) - 随机对OOB样本的特征i进行置换，计算Eoob(i)(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 什么是OOB？随机森林中OOB是如何计算的，它有什么优缺点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、每一次训练基模型g1，没有被Bagging过程选中的资料 \n",
    "\n",
    "2、1/e * N\n",
    "\n",
    "2、自我验证方法（类似留一法）xn -> g -> G -> xn误差相加平均"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF和GBDT之间的区别和联系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBDT和XGBoost ✔️✔️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBDT是先根据初始模型计算伪残差，之后建立一个学习器来解释伪残差，该学习器是在梯度方向上减少残差。再将该学习器乘上权重系数(学习速率)和原来的模型进行线性组合形成新的模型。这样反复迭代就可以找到一个使损失函数的期望达到最小的模型。\n",
    "\n",
    "1、GBDT如何选择特征：\n",
    "\n",
    "C&RT如何选择特征：\n",
    "\n",
    "    遍历每个特征\n",
    "    \n",
    "        遍历每个切分点\n",
    "        \n",
    "        计算当前切分点的回归误差\n",
    "        \n",
    "        如果是最小，记录特征+切分点\n",
    "\n",
    "2、GBDT如何用于分类：\n",
    "\n",
    "    回归：残差\n",
    "    \n",
    "    分类：类别离散化Onehot： 每个类别每次训练对应一颗🌲（类别 * 迭代次数）\n",
    "    \n",
    "    1、针对样本X的可能的每个类训练一个C&RT树 类1，类2，类3 【0，1，0】\n",
    "    \n",
    "    2、同时训练3课树，（x,0）（x,1）(x,0)，得到预测值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、传统的GBDT在优化的时候只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，得到一阶和二阶导数\n",
    "实际上使用二阶泰勒展开是为了xgboost能够【自定义loss function】；因为任何损失函数只要二阶可导即能【复用】陈佬所做的关于最小二乘法的任何推导。而且泰勒的本质是尽量去模仿一个函数，我猜二阶泰勒展开已经足以近似大量损失函数了，典型的还有基于分类的对数似然损失函数\n",
    "\n",
    "这样同一套代码就能完成回归或者分类了，而不是每次都推导一番，重写训练代码。\n",
    "\n",
    "\n",
    "> 建树分裂的好坏和损失函数并没有直接挂钩\n",
    "\n",
    "需要注意一点，其实损失函数不同，所假设的误差的所服从的分布也是不同的，平方误差的话，应该假设误差服从高斯分布，且最优值应该是均值；如果是绝对误差的话，应该假设误差服从拉普拉斯分布，且最优值应该是中位值\n",
    "\n",
    "损失函数：MSE，MAE，logloss，指数损失\n",
    "F初始化不一样\n",
    "计算叶子结点分数 - 叶子节点的取值和所选择的loss function有关。对于不同的Loss function，叶子节点的值也不一样\n",
    "\n",
    "\n",
    "建树分裂的好坏和损失函数挂钩\n",
    "\n",
    "不一样，计算叶子结点分数 - 推导，xgboost叶子节点取值的表达式更为简洁\n",
    "\n",
    "\n",
    "2、正则项：🌲的复杂度，减少过拟合\n",
    "\n",
    "    🌲分裂时增益：正则项里叶子节点数T的系数，所以xgboost在优化目标函数的同时相当于做了预剪枝\n",
    "    \n",
    "     lambda，是正则项里leaf score的L2模平方的系数，对leaf score做了平滑，也起到了防止过拟合的作用，这个是传统GBDT里不具备的特性。\n",
    "\n",
    "3、共同点：缩减：XGBoost在进行完一次迭代时，会将叶子节点的权值乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。\n",
    "\n",
    "4、每一次branch列抽样：随机森林的特点：防止过 拟合，还能减少计算\n",
    "\n",
    "子抽样：每轮计算可以不使用全部样本, 以减少过拟合\n",
    "\n",
    "5、缺失值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xgboost如何寻找最优特征，有放回/无放回"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、寻找最优特征：\n",
    "\n",
    "xgboost在训练的过程中给出各个特征的增益评分，最大增益的特征会被选出来作为分裂依据, 从而记忆了每个特征对在模型训练时的重要性 -- 从根到叶子中间节点涉及某特征的次数作为该特征重要性排序\n",
    "\n",
    "2、xgboost属于boosting集成学习方法, 样本是不放回的，因而每轮计算样本不重复"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost怎么给特征评分？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练的过程中，通过Gini指数选择分离点的特征，一个特征被选中的次数越多，那么该特征评分越高。\n",
    "\n",
    "Xgboost获取特征评分的方法：get_fscore\n",
    "\n",
    "Importance type can be defined as:\n",
    "\n",
    "    get_fscore： 'weight' - the number of times a feature is used to split the data across all trees.\n",
    "   \n",
    "                   所有的🌲中某个特征被用来划分数据集的次数\n",
    "   \n",
    "    get_score()  'gain' - the average gain of the feature when it is used in trees\n",
    "    \n",
    "                   特征被用来划分时的平均增益\n",
    "       \n",
    "    'cover' - the average coverage of the feature when it is used in trees\n",
    "     \n",
    "                   特征重要性使用特征在作为划分属性时对样本的覆盖度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xgboost调参"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调参过程：\n",
    "\n",
    "1、选择较高的学习速率(learning rate)。一般情况下，学习速率的值为0.1。但是，对于不同的问题，理想的学习速率有时候会在0.05到0.3之间波动。选择对应于此学习速率的理想决策树数量。XGBoost有一个很有用的函数“cv”，这个函数可以在每一次迭代中使用交叉验证，并返回理想的决策树数量。\n",
    "\n",
    "2、对于给定的学习速率和决策树数量，进行决策树特定参数调优(max_depth, min_child_weight, gamma, subsample, colsample_bytree)。在确定一棵树的过程中，我们可以选择不同的参数\n",
    "\n",
    "3、xgboost的正则化参数的调优。(lambda, alpha)。这些参数可以降低模型的复杂度，从而提高模型的表现。\n",
    "\n",
    "4、降低学习速率，确定理想参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans ✔️✔️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、适合连续值，不适合离散特征（使生成的每个聚类内紧凑，类间独立）\n",
    "\n",
    "优点：1、简单，快速\n",
    "\n",
    "缺点：必须给出K | 不同的初始化质心点，结果受初始值影响局部最优的迭代方法，每次可能结果不一样\n",
    "\n",
    "停止条件：1、同前一次迭代相比质心不变  2、达到最大迭代次数  3、同前一次迭代相比很少的点改变其类别\n",
    "\n",
    "K值\n",
    "\n",
    "最大迭代次数\n",
    "\n",
    "n_init：用不同的初始化质心运行算法的次数。由于K-Means是结果受初始值影响的局部最优的迭代算法，因此需要多跑几次以选择一个较好的聚类效果，默认是10，一般不需要改"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 欧式距离和曼哈顿距离"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两个样本之间的相似度\n",
    "\n",
    "1、欧式距离：\n",
    "\n",
    "    定义：两个点之间的直线距离\n",
    "    \n",
    "    缺点：把样本的不同特征的量纲之间的差别同等看待. (房子的面积，房子卧室的数量）\n",
    "         \n",
    "         从距离的角度来考虑，欧氏距离认为两点之间始终可以通过直线到达\n",
    "    \n",
    "    适用于样本各特征度量标准统一的情况\n",
    "  \n",
    "2、曼哈顿距离\n",
    "    \n",
    "    定义：两点间直线距离在坐标轴上的投影之和, 考虑了实际因素\n",
    "\n",
    "    坐标（x1, y1）的点P1与坐标（x2, y2）的点P2的曼哈顿距离为：|x1 - x2| + |y1 - y2|\n",
    "    \n",
    "    缺点：样本的不同特征的量纲之间的差别同等看待\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans的复杂度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、时间复杂度 O(tKmmn) 迭代次数 * 簇的数量 * 样本数 * 特征的数量\n",
    "\n",
    "2、空间复杂度 O(mn * kn) m：样本数 n:特征数 k：簇的数量\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans的K选取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、按需选择：衣服尺码，游戏玩家档位划分\n",
    "\n",
    "2、肉眼观察\n",
    "\n",
    "3、手肘法：得到聚类中心点后 -> 计算属于该类的点到该类的距离 -> 所有中心点求和\n",
    "\n",
    "横轴：K 纵轴：距离  下降突然平缓 （适用于高维样本数据）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans++初始中心点的选取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化seeds基本思想：初始的聚类中心之间 距离尽可能的远\n",
    "\n",
    "    1、数据集合中随机一个点\n",
    "    \n",
    "    2、轮盘法\n",
    "    \n",
    "    3、循环，直到选出K个点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯【拼写检查】✔️✔️  【生成式模型】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、字符串之间的相似程度：莱文斯坦距离距离 增删改交换\n",
    "\n",
    "2、朴素贝叶斯：argmax正确单词 =  P（Julw| 正确单词）* P（正确单词）\n",
    "\n",
    "                                  编辑距离    语料库概率      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推导朴素贝叶斯分类 P(c|d)，文档 d（由若干 word 组成），求该文档属于类别 c 的概率， 并说明公式中哪些概率可以利用训练集计算得到"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✔️  每一个类别中特征分布：高斯先验（连续值），多项式先验（alpha从未出现单词，先验平滑因子 - 文档分类），伯努利先验（0-1分布）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 贝叶斯定理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "条件概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为什么朴素贝叶斯如此“朴素”？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简洁\n",
    "\n",
    "1、假定所有的特征在数据集中同等重要 + 相互独立\n",
    "\n",
    "2、不关注单词之间的顺序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解释朴素贝叶斯算法里面的先验概率、似然估计和边际似然估计？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、先验概率：P(类别） P（特征） 后验概率：P （类别 | 特征）\n",
    "\n",
    "2、最大似然估计：**换类别** P（特征|类别） = 文档中该单词出现的次数 / 文档中总的单词数\n",
    "\n",
    "未知单词：平滑 - 出现次数 + 1  P（特征 | 类别c） = c文档中该单词出现的次数 + 1 / c文档中总的单词数 + **词库中所有单词的个数**\n",
    "\n",
    "概率问题：模型、参数 -> 数据\n",
    "\n",
    "统计：数据 -> 模型、参数\n",
    "\n",
    "> 贝叶斯公式：一个本来就难以发生的事情，就算出现某个证据和他强烈相关，也要谨慎。证据很可能来自别的虽然不是很相关，但发生概率较高的事情\n",
    "\n",
    "> 概率函数：如果θ是已知确定的，x是变量：对于不同的样本点x，其出现概率是多少。\n",
    "\n",
    "> 似然函数：描述对于不同的模型参数，出现x这个样本点的概率是多少\n",
    "    \n",
    "> 最大似然估计MLE（先验：theta均匀分布）：P（x|theta）（硬币💰）已知数据 -> 假设分布 -> 数据情况出现的概率 -> 最大化\n",
    "\n",
    "> 最大后验概率估计MAP（考虑先验概率假设，先验theta = 0.5）：最大化P(x|theta)P(theta)\n",
    "P（theta）假设服从0.5，方差为0.1的高斯分布，此时theta = 0.558\n",
    "多做实验，改变似然函数 theta = 0.696\n",
    "\n",
    "**一个合理的先验概率假设是很重要的，数据中直接分析得到** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "距离 -> 距离最小的K个点 -> 类别出现频率最高的\n",
    "\n",
    "优点：精度高，对异常值不敏感\n",
    "\n",
    "缺点：每一次预测需要计算所有训练样本的距离 -> 计算复杂度高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN的K如何选取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉验证选取K，较小一点的值\n",
    "\n",
    "K小 -> 容易被数据中的噪声所干扰\n",
    "\n",
    "K大 -> 与实例较远的样本也会起作用 -> 预测不准确"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据归一化（或者标准化，注意归一化和标准化不同）的原因"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、去量纲，梯度下降 加速收敛\n",
    "\n",
    "2、计算欧几里得距离时提高准确度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 哪些机器学习算法不需要做归一化处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不需要：概率模型，🌲模型\n",
    "\n",
    "需要：LR，横坐标一个特征（0，1），纵坐标一个特征（0，10000）。损失函数等值线高椭圆，迭代很多次\n",
    "\n",
    "        归一化后，标准的园，直接迭代到圆心\n",
    "\n",
    "归一化：\n",
    "       1、去除量纲，为了使计算更方便，加速梯度下降收敛\n",
    "\n",
    "       2、每一个特征值被平等对待，计算欧几里得距离\n",
    "\n",
    "树模型：决策树，随机森林 -> 熵/基尼系数\n",
    "\n",
    "因为：不关系变量的值，关心变量的分布和变量之间的条件概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对于树形结构为什么不需要归一化？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🌲模型：概率模型\n",
    "\n",
    "分割时：数值的缩放不会影响，算的是按照该特征划分后的熵和基尼系数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概念 简要说说一个完整机器学习项目的流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、定义问题，业务需求，所需特征\n",
    "\n",
    "2、收集数据\n",
    "\n",
    "3、数据清洗\n",
    "\n",
    "4、探索性数据分析，特征选择，特征工程\n",
    "\n",
    "5、训练模型，调参 \n",
    "\n",
    "6、交叉验证，模型评估\n",
    "\n",
    "7、模型融合\n",
    "\n",
    "8、模型上线测试\n",
    "\n",
    "9、效果评估，迭代优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 监督学习和无监督学习的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "监督：有标记，训练样本 -> 对训练集样本外的数据进行分类预测\n",
    "\n",
    "无监督：未标记 -> 这些样本中的结构知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常见的损失函数  ✔️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 协方差和相关性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "协方差：\n",
    "两个变量的协方差：同向变化 （正）/反向变化 （负） df【列名】.cov（df【列名】）\n",
    "所有变量的协方差：df.cov()\n",
    "\n",
    "相关系数：相关系数也可以看成协方差：一种剔除了两个变量量纲影响、标准化后的特殊协方差。\n",
    "两个变量每单位变化时的相似程度。\n",
    "\n",
    "两个变量的相关系数：df【列名】.corr（df【列名】）\n",
    "所有变量的相关系数df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VC维的理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "某类模型 - \n",
    "\n",
    "数据数量的包容性，VC越高，包容性越强。\n",
    "\n",
    "模型的复杂程度，模型假设空间越大，VC越高\n",
    "\n",
    "shatter 线性模型   最多shatter 3组二维数据，会出现4组数据无法shatter\n",
    "\n",
    "机器学习可学习性\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 过拟合怎么解决 ✔️✔️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、数据集：增多训练数据\n",
    "\n",
    "2、正则化： L1、L2范数，适当增大正则参数\n",
    "\n",
    "3、特征：适当减少特征数和所用的特征组合\n",
    "\n",
    "4、模型：简化模型：决策树剪枝，神经网络参数\n",
    "\n",
    "5、集成方法：使用bagging，不同学习器投票，减少方差。比如随机森林\n",
    "\n",
    "6、交叉验证，通过交叉验证选择适当的模型参数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 欠拟合怎么解决"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、模型：尝试非线性模型，比如Kernel SVM，决策树，Random Forest\n",
    "\n",
    "2、特征：增加新的特征 或  **特征组合**；吴恩达课程：逻辑回归：x1,x2，增大假设空间\n",
    "\n",
    "3、正则化：如果有，可能适当减少正则化系数\n",
    "\n",
    "4、集成方法：boosting -> 每次迭代生成基模型，主要提升前一代表现不好的地方，降低偏差。林轩田：弱模型，感知机\n",
    "    \n",
    "    Boosting结合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 欠拟合和过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欠拟合：高偏差，模型没有很好地拟合样本         \n",
    "\n",
    "    学习曲线：训练集和测试集上的误差都很大。\n",
    "\n",
    "过拟合：高方差，模型过度拟合样本，泛化能力差   \n",
    "    \n",
    "    学习曲线：训练误差和测试误差之间差别很大。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "降低过拟合，损失函数后边：惩罚系数 + 特征的权重\n",
    "\n",
    "L1 Lasso回归\n",
    "\n",
    "L2 岭回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1和L2的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、L1 ：每一个特征权重的绝对值作限制  | 稀疏\n",
    "\n",
    "2、L2： 每一个特征权重的平方作限制      | 平滑\n",
    "\n",
    "区别：1、健壮性，L1 > L2，L1抗干扰； 异常值重要，放大，L2\n",
    "\n",
    "     2、内置特征选择：（等值线，梯度下降公式推导）！！ L1容易产生为0的权重，稀疏矩阵； L2权重为0的几率很小。\n",
    "     \n",
    "     3、稀疏矩阵：计算效率高，减少存储空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1和L2正则先验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/yoyodelphine/article/details/52888315"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贝叶斯的角度：正则化是为满足模型的参数 w 引入先验分布。 横轴 w ，纵轴概率密度。w 的先验分布来限制w的值\n",
    "\n",
    "L1：拉普拉斯先验：凸尖的，在0的时候概率最大，稀疏\n",
    "\n",
    "L2：高斯先验：   凸平滑，接近0的时候概率变换缓慢，产生趋近于0的值；\n",
    "\n",
    "对于大的w，概率极低。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求导，公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降法找到的一定是下降最快的方向么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、梯度下降法并不是下降最快的方向，它只是目标函数在当前的点的切平面（当然高维问题不能叫平面）上下降最快的方向。\n",
    "\n",
    "2、 梯度下降不一定能够找到全局的最优解，也有可能只是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## insight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 没有免费的午餐 【具体问题具体分析】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、 解释：对于一个学习算法A，若它在某些问题上比学习算法 B更好，则必然存在一些问题，在那里B比A好\n",
    "\n",
    "2、假设所有问题出现几率相同，实际应用中，不同的场景，会有不同的问题分布，所以，在优化算法时，针对具体问题进行分析，是算法优化的核心所在。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  奥卡姆剃刀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当有两个处于竞争地位的理论能得出同样的结论，那么简单的那个更好。"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py27",
   "language": "python",
   "name": "py27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "600px",
    "left": "243px",
    "top": "107px",
    "width": "268.991px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
