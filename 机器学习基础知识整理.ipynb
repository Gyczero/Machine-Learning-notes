{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习基础知识总结\n",
    "\n",
    "通过面试题进一步理解机器学习基础知识：）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 面试Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、不会就是不会：**沟通能力 + 启发解释 + 思路：解决问题的能力**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 已掌握机器学习算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、线性回归，逻辑回归 \n",
    "\n",
    "2、SVM\n",
    "\n",
    "3、决策树，随机森林，GBDT\n",
    "\n",
    "4、朴素贝叶斯\n",
    "\n",
    "5、KMeans\n",
    "\n",
    "6、集成方法 Bending、Bagging、Boosting、Stacking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据验证方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练集：训练模型\n",
    "\n",
    "测试集：评估模型的泛化能力\n",
    "\n",
    "验证集：用来选择模型的超参数，比如SVM中的C，r\n",
    "\n",
    "\n",
    "交叉验证方法：\n",
    "\n",
    "1、K折交叉验证：\n",
    "\n",
    "    1、训练集平均分成K份（10） \n",
    "    \n",
    "    2、取第K份作为验证集，剩下K-1份作为训练集 \n",
    "    \n",
    "    3、K次得到的分数平均 = 验证集的分数\n",
    "    \n",
    "    优点：验证集多 -> 有效避免过拟合和欠拟合的发生，更有说服力\n",
    "    \n",
    "2、留一法（leave one out）：\n",
    "\n",
    "    1、一个数据（验证集） +  其他所有的数据（训练集）| N次\n",
    "    \n",
    "    2、再求平均\n",
    "    \n",
    "    优点：模型几乎等于实际训练集训练出的模型\n",
    "    \n",
    "    缺点：1、计算成本大\n",
    "    \n",
    "    用处：SVM快速安全检查  Err <= （支撑向量的个数 / 总样本数量）\n",
    "\n",
    "\n",
    "调参：\n",
    "\n",
    "    网格搜索（K-Fold）：不同的参数组合分别做一次交叉验证，取验证分数最高的 = 最优参数\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 工程面试题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit和fit_transform的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据某种同一处理，标准化等\n",
    "\n",
    "1、先fit，再transform：可以拿到数据的变换（标准化等）的参数，在测试集做一次相同的变换处理\n",
    "\n",
    "2、fit_transform 快，一次性完成数据的变换。但如果训练集和测试集上用fit_transform，可能执行两套变换标准\n",
    "\n",
    "后transform（）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.julyedu.com/question/big/kp_id/23/ques_id/1050"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原因： 1、减少特征数量、降维，使模型泛化能力更强，减少过拟合\n",
    "\n",
    "      2、增强可解释性 【预测房价】\n",
    "      \n",
    "方法：\n",
    "      1、正则： L1正则化可以产生稀疏的模型\n",
    "      \n",
    "      2、去除方差小的特征。方差小 = 意味着每个样本这个特征基本上没啥太大区别，对结果影响小\n",
    "      \n",
    "      3、随机森林做特征选择：分支时 = 【分类】信息增益/基尼不纯度最大\n",
    "      \n",
    "      4、结合业务理解，探索性数据分析，可解释性更强 【预测房价，房价跟面积数值变量 = 散点图； 房屋材料评估 = 箱型图】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.julyedu.com/question/big/kp_id/23/ques_id/1052"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4C：\n",
    "\n",
    "正确性（异常值：1.5四分位距 | 双变量散点图）\n",
    "\n",
    "完整性（None空值： 1、空值的业务意义  2、空值限定上限 3、太多去掉 4、数值：中位数；类别：众数）\n",
    "\n",
    "创造性\n",
    "\n",
    "转换性（类别特征独热编码； 数值特征 1，2，3转化为类别特征； 连续特征离散化（有些模型：逻辑回归需要离散值，表达能力））\n",
    "\n",
    "皮尔逊相关系数矩阵 【预测房价seaborn】去除相关性高的特征之一\n",
    "\n",
    "数据正态分布为什么：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征选择方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper\n",
    "\n",
    "https://www.julyedu.com/question/big/kp_id/23/ques_id/1061"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据不平衡的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 选择分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "吴恩达：\n",
    "\n",
    "1、特征比数据量还大，很大： 稀疏 -> 线性分类器，LR + Linear Kernel SVM\n",
    "\n",
    "2、特征比较小，样本数量多 ：  +手工添加特征  +非线性分类器\n",
    "\n",
    "3、特征小，样本量一般： SVM + 高斯Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常见的优化算法，优缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 给你一个有1000列和1百万行的训练数据集。这个数据集是基于分类问题的。\n",
    "经理要求你来降低该数据集的维度以减少模型计算时间。你的机器内存有限。你会怎么做？（你可以自由做各种实际操作假设）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 给你一个数据集，这个数据集有缺失值，且这些缺失值分布在离中值有1个标准偏差的范围内。百分之多少的数据不会受到影响？为什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 给你一个癌症检测的数据集。你已经建好了分类模型，取得了96％的精度。为什么你还是不满意你的模型性能？你可以做些什么呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 给你分配了一个新的项目，是关于帮助食品配送公司节省更多的钱。问题是，公司的送餐队伍没办法准时送餐。结果就是他们的客户很不高兴。\n",
    "\n",
    "最后为了使客户高兴，他们只好以免餐费了事。哪个机器学习算法能拯救他们？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 给你一个数据集。该数据集包含很多变量，你知道其中一些是高度相关的。\n",
    "经理要求你用PCA。你会先去掉相关的变量吗？为什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA降维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在PCA中有必要做旋转变换吗？\n",
    "如果有必要，为什么？如果你没有旋转变换那些成分，会发生什么情况？\n",
    "\n",
    "https://www.julyedu.com/question/big/kp_id/23/ques_id/2381"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 面试题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单介绍下LR  ✔️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR = 逻辑回归\n",
    "\n",
    "线性二分类 | softmax多分类\n",
    "\n",
    "1、公式，模拟实际\n",
    "\n",
    "2、判别，判断时\n",
    "\n",
    "3、损失函数，最大似然函数\n",
    "\n",
    "4、梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# penalty: 正则化 l1 / l2\n",
    "# solver : {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’},"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逻辑回归为什么要对特征进行离散化。（独热编码）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、表达能力：LR 线性模型，原来分类线是一条直线y = kx。特征离散化后，原来有一个特征x， x按区间离散化为x_1,x_2,x_3, 相对于y对A的函数相当于分段线性函数。引入了非线性\n",
    "\n",
    "2、健壮性：减少异常值的干扰。 > 30是1，否则为0。如果出现异常值年龄为300，原来 y = kx，异常值对最后结果影响很大；\n",
    "\n",
    "    而离散化后，300也是1，对结果影响较少。\n",
    "\n",
    "3、稳定性：比如年龄x在（20 - 30）为一个区间，不会因为用户增长一岁变成一个完全不同的人\n",
    "\n",
    "4、离散 = 0 / 1传给逻辑回归模型，稀疏向量。内积速度快，方便存储\n",
    "\n",
    "5、降低过拟合的风险。\n",
    "\n",
    "y = k1x1 + k2x2 ，如果x1的权重k1很大，模型过度依赖此特征x1，特征的一个微小变化可能会导致结果产生较大的变化。遇到新样本时可能因为特征的过度敏感得到错的分类结果。\n",
    "\n",
    "离散特征后，一个特征变成多个，权重也变为多个，之前连续特征对模型的影响力被弱化，降低了过拟合的风险。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR和SVM的联系和区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "共同点：\n",
    "\n",
    "    1、都用于分类，一般都用于二分类\n",
    "    2、不同的正则化项，如L1，L2\n",
    "    \n",
    "\n",
    "区别：\n",
    "\n",
    "    ❌1、损失函数不同（优化目标）：逻辑回归：最大似然函数，求w使出现样本情况的概率最大   SVM ：\n",
    "    \n",
    "    ❌2、优化方法不同：逻辑回归：梯度下降；  SVM：KKT条件，必要条件 + 拉格朗日乘子：对于an > 0，支撑向量，计算w和b\n",
    "    \n",
    "    2、几何意义：LR模型找超平面，尽量让所有的点远离这个超平面； SVM寻找最胖的超平面，让最靠近这个超平面的点远离。\n",
    "    \n",
    "    3、SVM只考虑支持向量（局部），也就是和分类最相关的少数点； 逻辑回归考虑全局。\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR和线性回归的区别和联系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "联系：都是广义的线性回归\n",
    "\n",
    "区别：1、线性回归计算的是值，实数范围内； 逻辑回归算的是概率【0，1】，减少预测范围，健壮性更好。\n",
    "\n",
    "     2、线性回归优化目标是最小化最小二乘函数，也就是每个点到超平面的距离最小\n",
    "     \n",
    "       逻辑回归的目标是最大化似然函数，也就是选取w，确定特征x下的概率，使产生训练集这样资料的概率最大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR和随机森林的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、LR是线性的，随机森林等树算法都是非线性的，\n",
    "\n",
    "2、LR全局的最优，🌲模型局部最优，切branch\n",
    "\n",
    "3、LR 梯度下降，去量纲，归一化。 🌲 模型数值不需要。\n",
    "\n",
    "4、优化过拟合的方法，LR正则化。随机森林：Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM、LR和决策树的对比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM的原理 ✔️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM 分类 kernel非线性 |  SVR回归\n",
    "\n",
    "1、标准SVM\n",
    "\n",
    "2、对偶SVM\n",
    "\n",
    "3、Kernel SVM\n",
    "\n",
    "4、"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 说说你知道的核函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、高斯Kernel  \n",
    "\n",
    "2、多项式核函数\n",
    "\n",
    "3、线性核函数\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 带Kernel的SVM为什么能分类非线性问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.julyedu.com/question/big/kp_id/23/ques_id/1566"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对偶的概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树 ✔️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 什么是决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "条件式的学习方法\n",
    "\n",
    "ID、C4.5分类\n",
    "\n",
    "CART 分类和回归\n",
    "\n",
    "1、模拟人决策的过程，整体是一个条件判断的集合。\n",
    "\n",
    "2、分枝节点：条件判断规则\n",
    "\n",
    "3、切分支时：特征空间划分成互不相交的部分。每次根据一个特征进行切分：\n",
    "\n",
    "    1、ID3中：按照不同的特征划分，选择划分后信息增益最大的那个特征进行切分 | 特征值有几种，切几个分支，多叉🌲\n",
    "    \n",
    "    2、CART： 按照不同特征划分，选择划分后基尼不纯度最小的那个特征进行切分 |  二叉🌲\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优点： 1、模型可解释性强（相比于SVM 高斯Kernel等）\n",
    "\n",
    "      2、轻易处理多分类问题 + 高效的非线性模型\n",
    "\n",
    "      3、特征：可以同时处理数值特征和类别特征\n",
    "      \n",
    "      4、缺失值：C&RT容易处理缺失值\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "缺点：1、容易过拟合，完全长成的🌲。\n",
    "\n",
    "解决方法：1、剪枝：前剪枝和后剪枝\n",
    "        \n",
    "        2、集成方法：bagging，随机森林\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常用参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# 默认C&RT\n",
    "\n",
    "# max_features：划分时考虑的最大特征数：控制决策树的生成时间，一般不变\n",
    "\n",
    "# 1、前剪枝\n",
    "\n",
    "# max_depth：   限制树的最大深度：样本多，特征多：10 - 100\n",
    "# min_samples_split： 样本数 < minXX：不会继续选择最优特征划分\n",
    "# max_leaf_nodes：最大叶子节点数\n",
    "# min_impurity_split：如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值\n",
    "\n",
    "\n",
    "# 2、后剪枝\n",
    "\n",
    "# min_samples_leaf：如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "递归的过程：\n",
    "\n",
    "1、接受训练集\n",
    "\n",
    "构建决策树函数：\n",
    "\n",
    "    一、：学习\n",
    "    \n",
    "    1、【学习分支】b(x)\n",
    "    \n",
    "    2、根据分支划分训练集 【分支数量】\n",
    "    \n",
    "    3、构建子树 <- 根据不同划分后的训练集\n",
    "    \n",
    "    4、返回整体的树🌲\n",
    "\n",
    "    \n",
    "    二：【停止条件】：\n",
    "    \n",
    "        返回叶子节点【基础假设】\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  具体关键的4个步骤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、分支数量：ID3 特征值数量     \n",
    "\n",
    "           C&RT：2 二叉树\n",
    "\n",
    "2、基础假设：返回常数。 \n",
    "\n",
    "            分类：最多的类别；\n",
    "            \n",
    "            回归：yn的平均值\n",
    "\n",
    "3、学习分支：看一个特征\n",
    "\n",
    "          1、ID3：特征每个值切割，信息增益最大  \n",
    "         \n",
    "          熵：信息的混乱程度 -SUM(pi * logPi)  信息增益：划分前熵 - 划分后每一个数据集的熵的和\n",
    "          \n",
    "          信息增益：对于多值特征（user_id），按照信息增益切分各部分几乎都是纯分类（熵最小为0），但没有意义。\n",
    "          \n",
    "          一般倾向于选择特征值多的特征切割\n",
    "          \n",
    "          \n",
    "          \n",
    "          2、C4.5信息增益比：信息增益 * 惩罚系数\n",
    "          \n",
    "          信息增益比本质： 是在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。\n",
    "          \n",
    "          惩罚系数：数据集以某一特征作为划分时熵的倒数：特征取值较小，倒数大；； 特征取值较大，倒数小\n",
    "          \n",
    "          【【【【选择】】】】：从候选特征中找出信息增益 > 平均水平 -> 选择信息增益比最高的\n",
    "          \n",
    "          \n",
    "\n",
    "\n",
    "         3、C&RT：切一刀分成两份\n",
    "          \n",
    "          特征类型（切什么）：\n",
    "          \n",
    "          离散特征：决策子集，二分离散特征。A1，A2，A3 =》 （A1A2，A3） （A1，A2A3）（A1A3，A2）\n",
    "          \n",
    "          连续特征：连续特征从小到大排序 -> 每一个中位点 -> 分支从中位点两侧分开 -> 计算判断标准 -> 最合理切分\n",
    "          \n",
    "          \n",
    "          切的判断标准（怎么切）：\n",
    "          \n",
    "          分类：基尼不纯度：数据集中随机抽取两个样本，类别不一致的概率 | 好处：计算简单\n",
    "          \n",
    "          回归：回归误差：SUM（每一个y减去整体的平均值的平方）\n",
    "          \n",
    "          \n",
    "          \n",
    "4、停止条件：\n",
    "\n",
    "     1、所有训练数据子集被正确分类\n",
    "\n",
    "     Ein = 0； 过拟合\n",
    "     \n",
    "     2、前剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树正则化：剪枝 （交叉验证）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、前剪枝：让决策树提前停止\n",
    "\n",
    "优缺点：简洁，节省计算 +  高度和阈值参数设置困难 + 可能欠拟合\n",
    "    \n",
    "       1、设置🌲的高度限制\n",
    "       \n",
    "       2、叶子结点的个数 < 【阈值】\n",
    "\n",
    "       3、信息增益/基尼不纯度 < 【阈值】\n",
    "       \n",
    "       4、每次划分时判断验证集的精度：如果验证集精度提升 -> 划分； 如果验证集精度不提升 -> 不划分 （基于贪心）\n",
    "       \n",
    "\n",
    "2、后剪枝：\n",
    "\n",
    "优缺点：得到完全长成的🌲后【自底向上】对分支结点进行考察处理 ，效果好  +  耗计算资源，时间开销大\n",
    "\n",
    "       1、【自底向上】考察每一个分支结点\n",
    "       \n",
    "       2、分支结点类别 = 对应子树中大多数训练样本所属的类别代替\n",
    "       \n",
    "       3、评估验证集的准确度：提高 -> 剪枝；；不提高 -> 不剪枝\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测中的缺失值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C&RT🌲中包括 **代理分支**：\n",
    "\n",
    "    1、训练时为每个特征找替代特征，b1(x)、b2(x)存起来：用替代特征做切割时跟原来类似\n",
    "    \n",
    "    2、预测时允许特征存在缺失值，使用替代特征代替"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 集成方法 ✔️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 说一下Adaboost，权值更新公式。当弱分类器是Gm时，每个样本的的权重是w1，w2...，请写出最终的决策公式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging、Boosting、Blending、Stacking的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机森林 ✔️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bagging + 决策树\n",
    "\n",
    "三个延伸：\n",
    "\n",
    "1、特征端随机抽取\n",
    "\n",
    "2、投影矩阵\n",
    "\n",
    "3、"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机森林如何处理缺失值？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机森林如何评估特征重要性？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF和GBDT之间的区别和联系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.julyedu.com/question/big/kp_id/23/ques_id/1531"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 什么是OOB？随机森林中OOB是如何计算的，它有什么优缺点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBDT和XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为什么Xgboost要用泰勒展开"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xgboost如何寻找最优特征，有放回/无放回"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost怎么给特征评分？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 欧式距离和曼哈顿距离"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两个样本之间的相似度\n",
    "\n",
    "1、欧式距离：\n",
    "\n",
    "    定义：两个点之间的直线距离\n",
    "    \n",
    "    缺点：把样本的不同特征的量纲之间的差别同等看待. (房子的面积，房子卧室的数量）\n",
    "         \n",
    "         从距离的角度来考虑，欧氏距离认为两点之间始终可以通过直线到达\n",
    "    \n",
    "    适用于样本各特征度量标准统一的情况\n",
    "  \n",
    "2、曼哈顿距离\n",
    "    \n",
    "    定义：两点间直线距离在坐标轴上的投影之和, 考虑了实际因素\n",
    "\n",
    "    坐标（x1, y1）的点P1与坐标（x2, y2）的点P2的曼哈顿距离为：|x1 - x2| + |y1 - y2|\n",
    "    \n",
    "    缺点：样本的不同特征的量纲之间的差别同等看待\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans初始中心点的选取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans的复杂度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、时间复杂度 O(tKmmn)\n",
    "\n",
    "2、空间复杂度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans的K选取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.julyedu.com/question/big/kp_id/23/ques_id/2119"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据归一化（或者标准化，注意归一化和标准化不同）的原因"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、去量纲，梯度下降 加速收敛\n",
    "\n",
    "2、计算欧几里得距离时提高准确度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 归一化的概念"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 哪些机器学习算法不需要做归一化处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不需要：概率模型，🌲模型\n",
    "\n",
    "需要：LR，横坐标一个特征（0，1），纵坐标一个特征（0，10000）。损失函数等值线高椭圆，迭代很多次\n",
    "\n",
    "        归一化后，标准的园，直接迭代到圆心\n",
    "\n",
    "归一化：\n",
    "       1、去除量纲，为了使计算更方便，加速梯度下降收敛\n",
    "\n",
    "       2、每一个特征值被平等对待，计算欧几里得距离\n",
    "\n",
    "树模型：决策树，随机森林 -> 熵/基尼系数\n",
    "\n",
    "因为：不关系变量的值，关心变量的分布和变量之间的条件概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对于树形结构为什么不需要归一化？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🌲模型：\n",
    "\n",
    "分割时：数值的缩放不会影响，算的是按照\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标准化与归一化的区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概念 简要说说一个完整机器学习项目的流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、定义问题，业务需求，所需特征\n",
    "\n",
    "2、收集数据\n",
    "\n",
    "3、数据清洗\n",
    "\n",
    "4、探索性数据分析，特征选择，特征工程\n",
    "\n",
    "5、训练模型，调参 \n",
    "\n",
    "6、交叉验证，模型评估\n",
    "\n",
    "7、模型融合\n",
    "\n",
    "8、模型上线测试\n",
    "\n",
    "9、效果评估，迭代优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 监督学习和无监督学习的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "监督：有标记，训练样本 -> 对训练集样本外的数据进行分类预测\n",
    "\n",
    "无监督：未标记 -> 这些样本中的结构知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常见的损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 协方差和相关性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 判别式模型和生成式模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性分类器和非线性分类器的区别及优劣"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最小二乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VC维的理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.julyedu.com/question/big/kp_id/23/ques_id/2117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 过拟合怎么解决 ✔️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、数据集：增多训练数据\n",
    "\n",
    "2、正则化： L1、L2范数，适当增大正则参数\n",
    "\n",
    "3、特征：适当减少特征数和所用的特征组合\n",
    "\n",
    "4、模型：简化模型：决策树剪枝，神经网络参数\n",
    "\n",
    "5、集成方法：使用bagging，不同学习器投票，减少方差。比如随机森林\n",
    "\n",
    "6、交叉验证，通过交叉验证选择适当的模型参数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 欠拟合怎么解决"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、模型：尝试非线性模型，比如Kernel SVM，决策树，Random Forest\n",
    "\n",
    "2、特征：增加新的特征 或  **特征组合**；吴恩达课程：逻辑回归：x1,x2，增大假设空间\n",
    "\n",
    "3、正则化：如果有，可能适当减少正则化系数\n",
    "\n",
    "4、集成方法：boosting -> 每次迭代生成基模型，主要提升前一代表现不好的地方，降低偏差。林轩田：弱模型，感知机\n",
    "    \n",
    "    Boosting结合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 欠拟合和过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欠拟合：高偏差，模型没有很好地拟合样本         \n",
    "\n",
    "    学习曲线：训练集和测试集上的误差都很大。\n",
    "\n",
    "过拟合：高方差，模型过度拟合样本，泛化能力差   \n",
    "    \n",
    "    学习曲线：训练误差和测试误差之间差别很大。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "降低过拟合，损失函数后边：惩罚系数 + 特征的权重\n",
    "\n",
    "L1 Lasso回归\n",
    "\n",
    "L2 岭回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1和L2的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、L1 ：每一个特征权重的绝对值作限制  | 稀疏\n",
    "\n",
    "2、L2： 每一个特征权重的平方作限制      | 平滑\n",
    "\n",
    "区别：1、健壮性，L1 > L2，L1抗干扰； 异常值重要，放大，L2\n",
    "\n",
    "     2、内置特征选择：（等值线，梯度下降公式推导）！！ L1容易产生为0的权重，稀疏矩阵； L2权重为0的几率很小。\n",
    "     \n",
    "     3、稀疏矩阵：计算效率高，减少存储空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1和L2正则先验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/yoyodelphine/article/details/52888315"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贝叶斯的角度：正则化是为满足模型的参数 w 引入先验分布。 横轴 w ，纵轴概率密度。w 的先验分布来限制w的值\n",
    "\n",
    "L1：拉普拉斯先验：凸尖的，在0的时候概率最大，稀疏\n",
    "\n",
    "L2：高斯先验：   凸平滑，接近0的时候概率变换缓慢，产生趋近于0的值；\n",
    "\n",
    "对于大的w，概率极低。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求导，公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降法找到的一定是下降最快的方向么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、梯度下降法并不是下降最快的方向，它只是目标函数在当前的点的切平面（当然高维问题不能叫平面）上下降最快的方向。\n",
    "\n",
    "2、 梯度下降不一定能够找到全局的最优解，也有可能只是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机梯度下降问题和挑战18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 牛顿法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## insight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 没有免费的午餐 【具体问题具体分析】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、 解释：对于一个学习算法A，若它在某些问题上比学习算法 B更好，则必然存在一些问题，在那里B比A好\n",
    "\n",
    "2、假设所有问题出现几率相同，实际应用中，不同的场景，会有不同的问题分布，所以，在优化算法时，针对具体问题进行分析，是算法优化的核心所在。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯【拼写检查】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推导朴素贝叶斯分类 P(c|d)，文档 d（由若干 word 组成），求该文档属于类别 c 的概率， 并说明公式中哪些概率可以利用训练集计算得到"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 贝叶斯定理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为什么朴素贝叶斯如此“朴素”？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解释朴素贝叶斯算法里面的先验概率、似然估计和边际似然估计？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN的K如何选取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 缺失值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 怎么理解决策树、xgboost能处理缺失值？而有的模型(svm)对缺失值比较敏感。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "600px",
    "left": "243px",
    "top": "107px",
    "width": "269px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
